\documentclass[12pt]{article}
\usepackage{amssymb,amsmath, amsthm, upgreek, mathrsfs, bussproofs, stmaryrd}
\usepackage{color}
\usepackage{tikz}
\usepackage{epigraph}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\author{Owain West}
\title{Intro Logic for Computer Scientists and Philosophers}
\setlength{\parskip}{1em}
\setlength\parindent{0pt}
\setlength{\epigraphwidth}{\textwidth}
\renewcommand{\textflush}{flushepinormal}
\begin{document}
\pagenumbering{gobble}
\maketitle
\newpage
\tableofcontents
\newpage
\pagenumbering{arabic}

\part*{To the Reader}
This book is meant to be used for a one-semester short course serving as an introduction to formal logic for technically-minded and philosophicall-minded students alike. As such, it aims to cover material which is pertinent to both audiences. Students who are comfortable with the introductory material should feel free to skip ahead. 

\newpage
\part{Introduction}
\epigraph{This conviction of the solvability of every mathematical problem is a powerful incentive to the worker. We hear within us the perpetual call: There is the problem. Seek its solution. You can find it by pure reason, for in mathematics there is no \emph{ignoramibus}.
\vspace*{5mm}

\hspace*{\fill} David Hilbert, \emph{Mathematical Problems}, 1900}

\section{What?}
A single definition of logic would be hard to give. To some, logic is rigorous the study of truth. To others, it is the study of proof. Others approach logic as a source for mathematical foundations, and other still approach logic as a means of understanding computation. 

In a way, all of these characterizations are fair. As it stands now, there are four main branches of study in logic: \emph{model theory} (``the study of truth''), \emph{proof theory} (``the study of proof''), \emph{set theory} (``the study of foundations''), and \emph{computability theory} (``the study of computation''). 


Mathematical Logic is distinguished from Mathematics proper in that it is concerned primarily with \emph{metamathematics}, that is, treating statements about mathematics as objects of mathematics themselves. In particular, notions like ``truth'' and ``provability'' are formalized, allowing meta-properties to be studied in a rigorous way. 


\section{Why?}

\section{Mathematical Preliminaries}
The only hard prerequisite for learning about mathematical logic is a willingness to think. The actual mathematical preliminaries necessary are simple enough that we can cover them quickly here; readers who think they need more background knowledge before beginning Chapter 1 should check the list of Suggested Readings. 

\subsection{Sets}
There is an intuitive, naive way to talk about sets as well as a rigorous, detailed way to define them. We take the naive approach here, as it will be (mostly) sufficient for our needs. 

A \emph{set} is, intuitively, simply unordered collection of objects (which are called the \emph{elements} of the set). For example, we might have a set $WeekDays$ whose members are \emph{Monday, Tuesday, Wednesday, Thursday, Friday}, or a set $EvenPrimes$ whose only member is the integer 2. To say that an object $a$ is an element of a set $A$, we write $a \in A$, which can be read as ``$a$ is an element of $A$'' or ``$a$ is in $A$''. 

Sets can be specified \emph{extensionally} (by listing the elements of the set) or \emph{intensionally} (by specifying some property which defines the members of the set). Consider the set (call it $P$) of all prime numbers less than 20. Extensionally, this is
\[
  P = \{2, 3, 5, 7, 11, 13, 17, 19\}
\]
The $\{,\}$ brackets indicate that the elements listed between are the members of the set in question. 

Intensionally, $P$ is
\[
  P = \{n\ |\ 0 < n < 20 \textbf{ and } n \text{ is prime.}\}
\]
To specify a set $S$ in this manner, we write $S = \{\text{placeholder }| \text{ statement about placeholder}\}$, and then $S$ is the collection of all objects such that the ``statement about placeholder'' holds of that object\footnote{This creation pattern for sets is called the \emph{naive comprehension principle}. The fact that ``naive'' is part of its name might set alarm bells ringing in your head, and for good reason. It turns out that this principle is actually contradictory - allowing sets to be defined by arbitrary properties leads to contradiction, and actually precipitated the biggest crisis of foundation in the history of mathematics, which itself resulted in rapid development in Logic. For our current purposes, this principle is fine. We'll discuss it in more detail later.}. 

Sets are \emph{unordered} and \emph{disregard multiplicity}. For example
\[
  \{1, 2, 3\} = \{3, 2, 1\}
\]
and
\[
  \{1\} = \{1, 1\}
\]
are both correct assertions; in both cases, the sets actually \emph{are equal} because neither order nor multiplicity matters for a set. Sets $A, B$ are equal if and only (henceforth abbreviated iff) if they have the same elements (multiplicity notwithstanding). 

For a set $A$, we write $|A|$ for the number of elements of $A$ (called the \emph{cardinality} of $A$). For finite sets, $|A|$ can be easily computed by counting how many elements $A$ has. For example
\[
  A = \{a, b, c\} \implies |A| = 3
\]
\[
  B = \{a, a, b, c\} \implies |B| = 3
\]
\[
  C = \{n\ |\ n \text{ is a positive integer no greater than }20\} \implies |C| = 20
\]

For sets $A, B$, the \emph{intersection} of $A$ and $B$, written $A \cap B$, is the collection of elements $x$ such that $x \in A$ and $x \in B$. For example
\[
  A = \{1, 2, 3\}, B = \{3, 4, 5\} \implies A \cap B = \{3\}
\]

The \emph{union} of sets $A, B$ (written $A \cup B$) is the collection of elements $x$ such that $x\in A$ or $x \in B$ (or both; in general, I will use ``$p$ or $q$'' to mean ``either $p$ or $q$ or both $p$ and $q$'', which is the standard interpretation of ``or'' in logic. I will say ``$p$ exclusive or $q$'' or ``either $p$ or $q$'' when I wish to exclude the case that both $p, q$ are true). For example
\[
  A = \{1, 2, 3\}, B = \{3, 4, 5\} \implies A \cup B = \{1, 2,3 , 4, 5\}
\]

Sets can be subtracted as well. For $A, B$, we write $A - B$ for the set of elements which are in $A$ but not in $B$. For example
\[
  A = \{1, 2, 3\}, B = \{3, 4, 5\} \implies A - B = \{1, 2\}
\]

The \emph{symmetric difference}\footnote{The $:=$ symbol is used to mean ``defined to be''.} $A \triangle B$ is defined as $A \triangle B := (A-B)\cup(B-A)$. For example, 
\[
  A = \{1, 2, 3\}, B = \{3, 4, 5\} \implies A \triangle B = \{1, 2, 4, 5\}
\]

We can also take the product of sets. For sets $A, B$ we have
\[
  A \times B = \{(a, b)\ |\ a \in A, b \in B \}
\]
which says that the elements of $A \times B$ are ordered pairs $(a, b)$ with $a \in A$ and $b \in B$. 

We say that $A$ is a \emph{subset} of $B$ iff every element of $A$ is also an element of $B$. In this case, we write $A \subseteq B$. Note that for every set $X$, we have $X \subseteq X$. If $A$ is a subset of $B$ but we want to specify additionally that $A$ is not equal to $B$, we write $A \subset B$ and say that ``$A$ is a proper subset of $B$''. For example, let
\[
  A = \{1, 2, 3\}, B = \{1, 2, 3\}, C = \{1, 2, 3, 4\}
\]
Then we have
\[
  A \subseteq B, A \subseteq C, A \subset C
\]
but we do not have
\[
  A \subset B
\]
because $A = B$. 

There are some special sets which deserve special attention. Some among them are
\begin{itemize}
  \item The emptyset, which is the unique set containing no elements
  \[
    \emptyset := \{\}
  \]
  \item The natural numbers
  \[
    \mathbb{N} := \{0, 1, 2, 3,...\}
  \]
  \item The integers
  \[
    \mathbb{Z} := \{...,-2,-1,0,1,2,...\}
  \]
  \item The rational numbers
  \[
    \mathbb{Q} := \{\frac{a}{b}\ |\ a, b \in \mathbb{Z}, b \neq 0\}
  \]
  \item The real numbers $\mathbb{R}$
\end{itemize}

\subsection{Functions and Relations}
\subsubsection*{Functions}
A function is a map from one set (called the domain) to another set (called the codomain) which maps every element in the domain to a single element of the codomain. For example, we might define the function $f$ as follows
\[
  f : \mathbb{R} \rightarrow \mathbb{R}
\]
\[
  x \mapsto x^2
\]
This says that $f$ is a function from domain $\mathbb{R}$ to codomain $\mathbb{R}$ which sends an element $x \in \mathbb{R}$ to $x^2 \in \mathbb{R}$. You may have seen this same function written before as 
\[
  f(x) = x^2
\]
Specifying the domain and codomain of a function is often beneficial, and so we will stick to the more verbose notation throughout this book. 

Functions are \emph{single-valued}, which means that if $f(x) = y$ and $f(x) = y'$, then $y = y'$. That is, each element of the domain maps to one element of the codomain. 

The \emph{image} of a function $f$ with domain $A$ and codomain $B$ is the set of all elements of the codomain which are mapped to by some element of the domain, that is
\[
   im(f) = \{b \in B\ |\ \text{There is an } a \in A \text{ with } f(a) = b\}
 \] 
Note that $im(f) \subseteq B$ and that it need not be the case that $im(f) = B$. For example, if we have 
\[
  f : \mathbb{R} \rightarrow \mathbb{R}
\]
\[
  x \mapsto x^2
\]
then the codomain of $f$ is $\mathbb{R}$ but $im(f) = \{r \in \mathbb{R}\ |\ r \geq 0\}$, the set of all nonnegative real numbers. 

We say that a function is \emph{injective}\footnote{Some books call this ``1-1'' or ``1-to-1''} iff no two members of the domain map to the same element of the codomain. We write $A \preceq B$ if there is an injection (an injective function) $f: A \rightarrow B$. It is simple to show that if $A \preceq B$ and $A, B$ are finite, then $|A| \leq |B|$ (try this as an exercise). This is true for infinite sets as well, but the proof is not as trivial. 

We say that a function is \emph{surjective}\footnote{Some books call this ``onto''} iff every element of the codomain is mapped to by some element of the domain (equivalently, if the image equals the codomain). As an exercise, show that if $A, B$ are finite and there is a surjection (a surjective function) from $A$ to $B$, then $|A| \geq |B|$. This is true for infinite sets as well, but the proof is not as trivial. 

We say that a function is \emph{bijective} (the function is a \emph{bijection}\footnote{Some books call this a ``1-to-1 correspondence''}) iff the function is both injective and surjective. This means that bijections associate every element of the domain with a unique element of the codomain, and every element of the codomain with an element of the domain. In this way, the domain and codomain are in a one-to-one correspondence. The results we have for finite sets (and our interpretation of what a bijection is) imply that if $A, B$ are finite and there is a bijection from $A$ to $B$, then $|A| = |B|$. This is true for infinite sets as well, but the proof is not as trivial. 

Technically, a set $A$ is finite iff there is a bijection 
\[
  f: A \rightarrow \{1, 2,...,n\}
\]
for some $n$. If there is no such bijection, then $A$ is \emph{infinite}. 

If there is a bijection from $A$ to $\mathbb{N}$, we say $A$ is \emph{countably infinite}. If $A$ is neither finite nor countable, we say $A$ is \emph{uncountable}. If $A$ is finite or countably infinite, we say that $A$ is \emph{countable}. 

\subsubsection*{Relations}
Relations are a generalization of functions which drop the requirement that the mapping be single-valued. A binary relation $R$ between a domain $A$ and a codomain $B$ is a set of pairs $(a, b)$ where $a \in A, b \in B$. In other words, a relation $R$ between $A$ and $B$ is some subset of $A \times B$). We will normally consider relations whose domain equals their codomain. In this case, a relation $R$ over a set $S$ is a subset of $S^2 = S \times S$. 

For example, with $S = \{1, 2\}$, $S \times S = \{(1, 1), (1, 2), (2, 1), (2, 2)\}$. We then might define some arbitrary relation $R$ on $S$ by $R = \{(1, 2), (1, 2)\}$.

Common mathematical operators can be thought of as relations - for example, $<$ is a relation on $\mathbb{N}$, since it specifies the subset of $\mathbb{N} \times \mathbb{N}$ for which the proposition ``$a$ is less than $b$'' holds.

We write $Rab$ (or, equivalently, $aRb$) to say that ``$a$ holds the relation $R$ to $b$''. The former notation is called ``prefix notation/Polish notation'' and is often used in logic/cs as it is more easily machine-readable and in accordance with our normal notation for functions (which, as we saw, are relations). The latter notation is called infix notation, and has the benefit of seemeing more natural as it's more widespread (we normally write $a < b$, for example, rather than ${<}(a, b)$). 

Notice that we identified relations with the \emph{extension of the relation}. To define a binary relation is simply to define which pairs $(a, b)$ for which the relation holds. 

Relations need not be just binary. For example, our normal interpretation of the $+$ symbol (along with equality) specifies a relation - we might think of the ternary relation $+abc$ as expressing that $a + b = c$. The extension of this relation is then a set of ordered triples $\{(a, b, c)\ |\ a + b = c\}$. 

As functions are relations, we can also think of a function $f: A \rightarrow B$ as being some subset of $A \times B$; the requirement that mappings be single valued ensures that for all $a$, there is only one $b$ such that $(a, b)$ is in the extension of the function. 

\subsection{Proof}
A \emph{proof} of a statement $p$ is some sequence of assertions, beginning with simple assertions called \emph{axioms} and ending with $p$ such that each statement follows logically from the previous statement. We will have much more to say about what constitutes an acceptable axiom or inference later. 

There are a few common proof techniques used when proving simple propositions. The first we consider is \emph{proof by contradiction}. 

\subsubsection*{Proof by Contradiction} Suppose we are asked to prove proposition $p$. Assume that $p$ is not true and show that this leads to a contradiction. Conclude that $p$ is true. 

By way of example, suppose we are asked to prove that there are infinitely many prime numbers. We might prove this as follows

\begin{proof}
Suppose towards a contradiction that there are finitely many prime numbers. If this were the case, we could list out all the primes as $p_1,p_2,...p_n$, where $p_n$ is the greatest prime. Consider the number $p = p_1 \cdot p_2 \cdot ... \cdot p_n + 1$. Clearly we have $p_i \nmid p$ for all $i$ (we write $a | b$ to mean $a$ divides $b$; similarly, $a \nmid b$ means $a$ does not divide $b$). So $p$ has no divisors except for itself and $1$, and so is also prime. But $p > p_n$, contradicting that $p_n$ was the greatest prime. So there are infinitely many prime numbers. 
\end{proof}

\subsubsection*{Proof by Induction} Suppose that you are asked to prove that a proposition holds for all natural numbers. You might do this as follows: show that it holds for 0, and show that if it holds for $n$, it holds for $n + 1$. This suffices to show that the proposition holds for all natural numbers (we have that it holds for 0, therefore it holds for 1, therefore it holds for 2, etc). This strategy is called \emph{proof by induction}. Proving tthat the proposition holds for 0 (or, some arbitrary base number) is called the \emph{base case} and proving the implication ``holds for $n$ implies holds for $n + 1$'' is called the \emph{inductive step}. 

By way of example, suppose we are asked to prove the well-known identity $\sum_1^ni = \frac{n(n+1)}{2}$. We might prove this as follows

\begin{proof}
BASE: let $n = 1$. Then $\sum_1^ni = 1 = (\frac{1(1+1)}{2})$ and so the base case holds. 

INDUCT: Suppose that $\sum_1^ni = \frac{n(n+1)}{2}$. We want to show that this holds for $n + 1$, that is, that $\sum_1^{n + 1}i = \frac{(n + 1)(n+2)}{2}$

We have that $\sum_1^ni = \frac{n(n+1)}{2}$. Then 
\[
  (\sum_1^ni) + (n + 1)= \frac{n(n+1)}{2} + (n + 1)
\]
Moving $n + 1$ inside the sum on the left hand side and expanding the right hand side, we get
\begin{align*}
(\sum_1^{n + 1}i) &= (n^2 + n)/2 + (2n + 2)/2 \\
&= (n^2 + 3n + 2)/2 \\
&= ((n+1)(n+2))/2
\end{align*}
which is what we wanted to show, and so we are done. 
\end{proof}

\subsubsection*{Direct Proof} Sometimes, there is no need for a fancy-schmancy proof technique, and a direct proof is the easiest way to go about things. For example, suppose we were asked to prove that $(A \cap (B \cup C)) \subseteq ((A \cap B) \cup (A \cap C))$ for arbitrary sets $A, B, C$. We might prove this as follows. 

\begin{proof}
Let $x \in (A \cap (B \cup C))$. Then $x \in A$ and $x \in (B \cup C)$. Then $x \in A$ and $x \in B$, or $x \in A$ and $x \in C$. If $x \in A$ and $x \in B$ then $x \in (A \cap B)$ so $x \in ((A \cap B) \cup (A \cap C))$. If $x \in A$ and $x \in C$ then $x \in (A \cap C)$ so $x \in ((A \cap B) \cup (A \cap C))$. 

So each element of $(A \cap (B \cup C))$ is an element of $((A \cap B) \cup (A \cap C))$, and we are done. 
\end{proof}

\subsection{Graphs}
A \emph{graph} is a tuple $G = (V, E)$ where $V$ is a set of ``vertices'' or ``nodes'', and $E$ is a binary relation (the edge relation) on $V$ (ie, $E \subseteq V \times V$). For example, if $G = (V, E)$ with $v = \{1, 2, 3\}$ and $E = \{(1, 2), (2, 1), (1, 3), (3, 1), (2, 3), (3, 2)\}$, we might draw this as

\begin{tikzpicture}
\node (v3) at (-2.5,2.5) {1};
\node (v1) at (-4.5,5) {2};
\node (v2) at (-0.5,6) {3};
\draw  (v1) edge (v2);->>
\draw  (v3) edge (v1);
\draw  (v2) edge (v3);
\end{tikzpicture}

where the straight lines are \emph{undirected edges} due to the fact that whenever we have an edge from $a$ to $b$ (ie, a pair $(a, b) \in E$) we also have an edge from $b$ to $a$ (ie, a pair $(b, a) \in E$). If all edges are of this form the graph is said to be \emph{undirected}. If this is not the case, the graph is said to be \emph{directed} and edges are drawn with arrowheads. 

Relations are often well represented as graphs; a relation $R$ on a set $S$ has a graph whose nodes are the elements of $S$ and which has an edge $(a, b)$ for each $(a, b) \in R$. 

\textbf{Neighbourhood}: the neighbourhood of a node $n$ of a graph $G$ is the set of all nodes which are adjacent to $n$ in $G$ (ie, $\{n'\ |\ nEn' \}$).

\textbf{Degree}: the degree os a node $n$ is the size of $n$'s neighbourhood. 

\textbf{k-Regular}: we say that a graph $G$ is $k$-regular iff every node $n \in G$ has degree $k$. If we take $E$ to be our edge relation, this can be schematized as 

Note that finite $2$-regular simple graphs are composed of a collection of disjoint cycles, and $1$-regular simple graphs are composed of a collection of disjoint pairs of elements. The normal ordering relation on the integers (ie, the graph $(\mathbb{Z}, <)$) is an acyclic 2-regular graph (draw the graph to verify). 

\newpage
\part{Propositional Logic}
\section{Language and Structure}
% Let's begin with an example. In highschool algebra, you likely learned that the vector space $\mathbb{R}^2$ was a \emph{model} for Euclidean Geometry. Similarly, you might know about Groups, which are \emph{models} of the group axioms, or about probability spaces, which are \emph{models} or axioms about probability. 

The first logical system (alternatively called a \emph{Language}) we consider is \emph{Propositional Logic}, which we will abbreviate $\mathcal{LP}$. To specify a language, we need to describe the following
\begin{itemize}
  \item The lexicon - ie what the valid ``words'' are in the language
  \item The syntax. This specifies how words can be connected together to form sentences. 
  \item The semantics. This is the relation between sentences (which are \emph{a priori} just meaningless sequences of symbols) and the meaning which a sentence prescribes. 
\end{itemize}
It is the interplay between syntax and semantics - formalism and meaning - that gives logic its power. 

\subsection*{Syntax}
A special subset of a language's lexicon is its \emph{signature}, denoted $\sigma$. For $\mathcal{LP}$, the signature can be any set of varaibles, which might look like
\[
  p, q, r, x, x', x_{1000}, \text{ etc}
\]
which we will use to express propositions. If we have signature $\sigma$, we write $\mathcal{LP}(\sigma)$ to indicate that we are building sentences in $\mathcal{LP}$ with signature $\sigma$. $\mathcal{LP}$'s lexicon also contains the distinguished symbols
\[
  \land, \vee, \rightarrow,  \leftrightarrow, \lnot
\]
called the \emph{logical connectives} which are not allowed to be part of any signature, as well as parentheses 
\[
  (, )
\]
which are also not allowed to be part of the signature. Finally there are the \emph{truth-values}
\[
  \top, \bot
\]
which will be explained later. 

An \emph{expression} of $\mathcal{LP}(\sigma)$ is a string of symbols from the the lexicon of $\mathcal{LP}(\sigma)$. The length of this expression is the number of symbols it has. 

As of yet, there is no notion of what a \emph{meaningful} sentence of $\mathcal{LP}(\sigma)$ might look like. To achieve that, we define the \emph{well-formed formulae (wffs)} of $\mathcal{LP}(\sigma)$. The following are wffs of $\mathcal{LP}(\sigma)$. 
\begin{itemize}
  \item $\bot$ is a wff as are all elements of $\sigma$
  \item If $\phi$ is a wff, so is $\lnot \phi$
  \item If $\phi, \psi$ are wffs, so are $(\phi \land \psi), (\phi \vee \psi), (\phi \rightarrow \psi)$, and $(\phi \leftrightarrow \psi)$.
\end{itemize}
Nothing else is a wff of $\mathcal{LP}(\sigma)$. 

For example, the following are wffs with $\sigma = \{p, q, r\}$
\[
  (p \rightarrow r) \rightarrow (p \rightarrow (q \rightarrow r))
\]
\[
  \lnot((p \vee \lnot p) \land (q \land \lnot q))
\]
whereas the following are not wffs
\[
  p((\bot
\]
\[
  q \rightarrow q \land r)
\]
This defines the syntax, and all we are left with is defining the semantics. 

\subsection*{Semantics}
We can associate wffs with meaningful statements by giving each of the propositional connectives an interpretation. We will speak of truth and falsity as \emph{truth values}, which we will write as $\top$ and $\bot$ respectively. The \emph{truth value} of a statement will be $\top$ if the statement is true, and will be $\bot$ otherwise. 

The truth-value of a wff depends on the truth-values assigned to the propositional symbols in the formula. We define how to propage truth-values up from propositions to wffs by way of \emph{truth tables}.

Suppose we want to know the truth-value of $(p \land q)$ from the truth vales of $p$ and $q$. The truth table below explains how:

\begin{table}[h!]
\centering
\caption{Conjunction (logical and)}
\begin{tabular}{l|l|l}
$p$    & $q$    & $p \land q$ \\ \hline
$\top$ & $\top$ &  $\top$     \\
$\top$ & $\bot$ &  $\bot$   \\
$\bot$ & $\top$ &  $\bot$   \\
$\bot$ & $\bot$ &  $\bot$   
\end{tabular}
\end{table}
This table says that if $p$ is true and $q$ is true, so is $p \land q$. Otherwise, $p \land q$ is false. That gives a natural interpretation of $p \land q$ as expressing ``$p$ and $q$''.

\begin{table}[h!]
\centering
\caption{Disjunction (logical or)}
\begin{tabular}{l|l|l}
$p$    & $q$    & $p \vee q$ \\ \hline
$\top$ & $\top$ & $\top$      \\
$\top$ & $\bot$ & $\top$    \\
$\bot$ & $\top$ & $\top$    \\
$\bot$ & $\bot$ & $\bot$    
\end{tabular}
\end{table}
This table says that $p \vee q$ is true iff at least one of $p$ or $q$ is true. That gives a natural interpretation of $p \vee q$ as expressing ``$p$ or $q$''.

\begin{table}[h!]
\centering
\caption{Negation (not)}
\label{my-label}
\begin{tabular}{l|l}
$p$     & $\lnot p$ \\ \hline 
$\top$  & $\bot$      \\
$\bot$  & $\top$    \\
\end{tabular}
\end{table}
This says that $\lnot p$ is true iff $p$ is false. This gives a natural interpretation of $\lnot p$ as expressing ``not $p$''.  

\begin{table}[h!]
\centering
\caption{Conditional (if... then)}
\label{my-label}
\begin{tabular}{l|l|l}
$p$    & $q$    & $p \rightarrow q$ \\ \hline
$\top$ & $\top$ & $\top$      \\
$\top$ & $\bot$ & $\bot$    \\
$\bot$ & $\top$ & $\top$    \\
$\bot$ & $\bot$ & $\top$    
\end{tabular}
\end{table}
This says that $p \rightarrow q$ is false only when $p = \top$ and $q = \bot$, and should be interpreted as saying ``if $p$, the $q$'' or more colloquially ``$p$ implies $q$''. In a sentence of the form $p \rightarrow q$, $p$ is called the \emph{antecedent} and $q$ is called the \emph{consequent}. 

The motivation for the first two lines of the truth table should be intuitive given the suggested interpretation. For the third line, consider the sentence ``if 4 is a multiple of 3 then 6 is a multiple of 3''. The antecedent of that sentence is false and its consequent is true, and we intuitively think that the whole sentence is true. Similarly, we also agree that a sentence like ``if 4 is a multiple of 3 then 5 is a multiple of 3'' is true, even though both antecedent and consequent are false. 

\begin{table}[h!]
\centering
\caption{Biconditional (iff)}
\label{my-label}
\begin{tabular}{l|l|l}
$p$    & $q$    & $p \leftrightarrow q$ \\ \hline
$\top$ & $\top$ & $\top$      \\
$\top$ & $\bot$ & $\bot$    \\
$\bot$ & $\top$ & $\bot$    \\
$\bot$ & $\bot$ & $\top$    
\end{tabular}
\end{table} 
This says that $p \leftrightarrow q$ is true iff $p, q$ have the same truth value. This gives a natural nterpretation of $p \leftrightarrow q$ as ``$p$ if and only if $q$''. 

\newpage
\part{First Order Logic}

\newpage
\part{Semantics}

\newpage
\part{Syntax}

\newpage
\part{Computation}

\newpage
\part{Complexity}

\newpage
\part{Philosophy}

\newpage
\part{Suggested Readings}
\end{document}
